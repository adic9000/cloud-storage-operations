groups:
- name: "pgs"
  rules:
    - alert: "CephPGsInactive"
      annotations:
        description: "{{ $value }} PGs have been inactive for more than 5 minutes in pool {{ $labels.name }}. Inactive placement groups are not able to serve read/write requests."
        summary: "One or more placement groups are inactive"
      expr: "ceph_pool_metadata * on(pool_id,instance) group_left() (ceph_pg_total - ceph_pg_active) > 0"
      for: "5m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.1"
        severity: "critical"
        type: "ceph_default"
    - alert: "CephPGsUnclean"
      annotations:
        description: "{{ $value }} PGs have been unclean for more than 15 minutes in pool {{ $labels.name }}. Unclean PGs have not recovered from a previous failure."
        summary: "One or more placement groups are marked unclean"
      expr: "ceph_pool_metadata * on(pool_id,instance) group_left() (ceph_pg_total - ceph_pg_clean) > 0"
      for: "15m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.2"
        severity: "warning"
        type: "ceph_default"
    - alert: "CephPGsDamaged"
      annotations:
        description: "During data consistency checks (scrub), at least one PG has been flagged as being damaged or inconsistent. Check to see which PG is affected, and attempt a manual repair if necessary. To list problematic placement groups, use 'rados list-inconsistent-pg <pool>'. To repair PGs use the 'ceph pg repair <pg_num>' command."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-damaged"
        summary: "Placement group damaged, manual intervention needed"
      expr: "ceph_health_detail{name=~\"PG_DAMAGED|OSD_SCRUB_ERRORS\"} == 1"
      for: "5m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.4"
        severity: "critical"
        type: "ceph_default"
    - alert: "CephPGRecoveryAtRisk"
      annotations:
        description: "Data redundancy is at risk since one or more OSDs are at or above the 'full' threshold. Add more capacity to the cluster, restore down/out OSDs, or delete unwanted data."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-recovery-full"
        summary: "OSDs are too full for recovery"
      expr: "ceph_health_detail{name=\"PG_RECOVERY_FULL\"} == 1"
      for: "1m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.5"
        severity: "critical"
        type: "ceph_default"
    - alert: "CephPGUnavilableBlockingIO"
      annotations:
        description: "Data availability is reduced, impacting the cluster's ability to service I/O. One or more placement groups (PGs) are in a state that blocks I/O."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-availability"
        summary: "PG is unavailable, blocking I/O"
      expr: "((ceph_health_detail{name=\"PG_AVAILABILITY\"} == 1) - scalar(ceph_health_detail{name=\"OSD_DOWN\"})) == 1"
      for: "1m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.3"
        severity: "critical"
        type: "ceph_default"
    - alert: "CephPGBackfillAtRisk"
      annotations:
        description: "Data redundancy may be at risk due to lack of free space within the cluster. One or more OSDs have reached the 'backfillfull' threshold. Add more capacity, or delete unwanted data."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-backfill-full"
        summary: "Backfill operations are blocked due to lack of free space"
      expr: "ceph_health_detail{name=\"PG_BACKFILL_FULL\"} == 1"
      for: "1m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.7.6"
        severity: "critical"
        type: "ceph_default"
    - alert: "CephPGNotScrubbed"
      annotations:
        description: "One or more PGs have not been scrubbed recently. Scrubs check metadata integrity, protecting against bit-rot. They check that metadata is consistent across data replicas. When PGs miss their scrub interval, it may indicate that the scrub window is too small, or PGs were not in a 'clean' state during the scrub window. You can manually initiate a scrub with: ceph pg scrub <pgid>"
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-not-scrubbed"
        summary: "Placement group(s) have not been scrubbed"
      expr: "ceph_health_detail{name=\"PG_NOT_SCRUBBED\"} == 1"
      for: "5m"
      labels:
        severity: "warning"
        type: "ceph_default"
    - alert: "CephPGsHighPerOSD"
      annotations:
        description: "The number of placement groups per OSD is too high (exceeds the mon_max_pg_per_osd setting).\n Check that the pg_autoscaler has not been disabled for any pools with 'ceph osd pool autoscale-status', and that the profile selected is appropriate. You may also adjust the target_size_ratio of a pool to guide the autoscaler based on the expected relative size of the pool ('ceph osd pool set cephfs.cephfs.meta target_size_ratio .1') or set the pg_autoscaler mode to 'warn' and adjust pg_num appropriately for one or more pools."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks/#too-many-pgs"
        summary: "Placement groups per OSD is too high"
      expr: "ceph_health_detail{name=\"TOO_MANY_PGS\"} == 1"
      for: "1m"
      labels:
        severity: "warning"
        type: "ceph_default"
    - alert: "CephPGNotDeepScrubbed"
      annotations:
        description: "One or more PGs have not been deep scrubbed recently. Deep scrubs protect against bit-rot. They compare data replicas to ensure consistency. When PGs miss their deep scrub interval, it may indicate that the window is too small or PGs were not in a 'clean' state during the deep-scrub window."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-not-deep-scrubbed"
        summary: "Placement group(s) have not been deep scrubbed"
      expr: "ceph_health_detail{name=\"PG_NOT_DEEP_SCRUBBED\"} == 1"
      for: "5m"
      labels:
        severity: "warning"
        type: "ceph_default"

